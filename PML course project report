Practice Machine Learning course project report

Ying Han

April 26, 2015

Data cleaning and pre-processing

Features:

There are 160 variables provided in the original datasets. Of these variables, 100 have a missing rate of ~98% which were thus removed from the analysis. For predictive analysis, 4 variables including outcome “classe”, row number “X”, subject identifier “user_name”, and measurement time stamp “cvtd_timestamp” were not considered as potential predictors, which left 56 variables for model building. Of the 56 features, one (“new_window”) is a binary variable and was thus recoded as 0/1, while the rest are continuous variables. To speed up the convergence in optimizing the objective function, I performed feature scaling by centering each variable on its mean and then scaling it to unit length. 

Outcome:

There are five possible classes of the outcome, labeled as A, B, C, D, and E. The features and outcomes were monitored on 6 subjects (by examining the variable “user_name”). As shown in Figure 1, the frequency of each class is relatively balanced across all 6 subjects, without any dominating class in the distribution. Therefore, I used overall accuracy instead of kappa statistics, which is more appropriate for skewed distributions, as the model performance metric.

 
Figure 1. Distribution of activity quality (classe) across users

 
Model building

Based on the pre-processed training dataset, I built the model using penalized multinomial logistic regression. This algorithm is appropriate for classification problems with multiple (>2) classes. By including a penalty term in the model, it incorporates feature selection (ie., reducing multicollinearity among features selected in the final model) and also controls the bias-variance tradeoff. To tune the hyperparameter in the penalty term, I used 5-time repeated 5-fold cross validation to prevent overfitting. 

To estimate the out-of-sample error, I split the original training dataset into a training set and a validation set using leave-one-subject-out strategy. In other words, data collected from one subject went to the validation set and the rest went to the training set, and this process was repeated on every one of the 6 subjects. Within each iteration, the training set was used to fit the model and the validation set was used to estimate the prediction accuracy. The out-of-sample error was estimated by averaging the confusion matrices over all subjects.

Performance evaluation

The model performance on each left-out subject and overall (weighted average) is shown in Table 1.  The out-of-sample accuracy is estimated to be 76%.

Table 1. Model performance on each subject and overall

 

Given the limited time, I only implemented one machine-learning algorithm (penalized multinomial logistic regression); if given more time, I would like to apply different algorithms, evaluate their performances using the above frameworks, and choose the one with the best performance to predict on the testing dataset.


