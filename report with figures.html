<p align="center">
    Practice Machine Learning course project report
</p>
<p align="center">
    Ying Han
</p>
<p align="center">
    April 26, 2015
</p>
<p>
    <strong>Data cleaning and pre-processing</strong>
</p>
<p>
    <u>Features</u>
    :
</p>
<p>
    There are 160 variables provided in the original datasets. Of these variables, 100 have a missing rate of ~98% which were thus removed from the analysis.
    For predictive analysis, 4 variables including outcome “classe”, row number “X”, subject identifier “user_name”, and measurement time stamp
    “cvtd_timestamp” were not considered as potential predictors, which left 56 variables for model building. Of the 56 features, one (“new_window”) is a
    binary variable and was thus recoded as 0/1, while the rest are continuous variables. To speed up the convergence in optimizing the objective function, I
    performed feature scaling by centering each variable on its mean and then scaling it to unit length.
</p>
<p>
    <u>Outcome</u>
    :
</p>
<p>
    There are five possible classes of the outcome, labeled as A, B, C, D, and E. The features and outcomes were monitored on 6 subjects (by examining the
    variable “user_name”). As shown in <strong>Figure 1</strong>, the frequency of each class is relatively balanced across all 6 subjects, without any
    dominating class in the distribution. Therefore, I used overall accuracy instead of kappa statistics, which is more appropriate for skewed distributions,
    as the model performance metric.
</p>
<p>
    <img
        src="file://localhost/Users/Serena/Library/Caches/TemporaryItems/msoclip/0/clip_image002.png"
        alt="Description: Macintosh HD:Users:Serena:Courses:Practical Machine Learning (Coursera):project:classe_by_user.pdf"
        height="299"
        width="299"
    />
</p>
<p>
    Figure 1. Distribution of activity quality (classe) across users
</p>
<strong>
    <br clear="all"/>
</strong>
<p>
    <strong> </strong>
</p>
<p>
    <strong>Model building</strong>
</p>
<p>
    Based on the pre-processed training dataset, I built the model using penalized multinomial logistic regression. This algorithm is appropriate for
    classification problems with multiple (&gt;2) classes. By including a penalty term in the model, it incorporates feature selection (ie., reducing
    multicollinearity among features selected in the final model) and also controls the bias-variance tradeoff. To tune the hyperparameter in the penalty term,
    I used 5-time repeated 5-fold cross validation to prevent overfitting.
</p>
<p>
    To estimate the out-of-sample error, I split the original training dataset into a training set and a validation set using leave-one-subject-out strategy.
    In other words, data collected from one subject went to the validation set and the rest went to the training set, and this process was repeated on every
    one of the 6 subjects. Within each iteration, the training set was used to fit the model and the validation set was used to estimate the prediction
    accuracy. The out-of-sample error was estimated by averaging the confusion matrices over all subjects.
</p>
<p>
    <strong>Performance evaluation</strong>
</p>
<p>
    The model performance on each left-out subject and overall (weighted average) is shown in <strong>Table 1</strong>. The out-of-sample accuracy is estimated
    to be 76%.
</p>
<p>
    <strong> </strong>
</p>
<p>
    Table 1. Model performance on each subject and overall
</p>
<p>
    <img
        src="file://localhost/Users/Serena/Library/Caches/TemporaryItems/msoclip/0/clip_image004.png"
        alt="Description: Macintosh HD:Users:Serena:Desktop:performance.png"
        height="96"
        width="220"
    />
</p>
<p>
    Given the limited time, I only implemented one machine-learning algorithm (penalized multinomial logistic regression); if given more time, I would like to
    apply different algorithms, evaluate their performances using the above frameworks, and choose the one with the best performance to predict on the testing
    dataset.
</p>
